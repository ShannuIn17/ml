{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find-S Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "data = DataFrame.from_csv('EnjoySport.csv')\n",
    "columnLength= data.shape[1]\n",
    "print (data.values)\n",
    "h = ['0']*(columnLength-1)\n",
    "hp=[]\n",
    "hn=[]\n",
    "for trainingExample in data.values:\n",
    "    if trainingExample[-1]!='no':\n",
    "        hp.append(list(trainingExample))\n",
    "    else:\n",
    "        hn.append(list(trainingExample))\n",
    "for i in range (len(hp)):\n",
    "    for j in range(columnLength-1):\n",
    "        if (h[j]=='0'):\n",
    "            h[j]=hp[i][j]\n",
    "        elif (h[j]!=hp[i][j]):\n",
    "            h[j]='?'\n",
    "        else:\n",
    "            h[j]=hp[i][j]\n",
    "print('\\nThe positive Hypotheses are:',hp)\n",
    "print('\\nThe negative Hypotheses are:',hn)\n",
    "print('\\nThe Maximally Specific Hypothesis h is:',h)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate-Elimination Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "data=DataFrame.from_csv('EnjoySport.csv')\n",
    "concepts=data.values[:,:-1]\n",
    "target=data.values[:,-1]\n",
    "def learn(concepts, target):\n",
    "    specific_h = concepts[0].copy()\n",
    "    general_h = [['?' for i in range(len(specific_h))] for i in range(len(specific_h))]\n",
    "    for i, h in enumerate(concepts):\n",
    "        if target[i] == \"yes\":\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] != specific_h[x]:\n",
    "                    specific_h[x] = '?'\n",
    "                    general_h[x][x] = '?'\n",
    "        if target[i] == \"no\":\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] != specific_h[x]:\n",
    "                    general_h[x][x] = specific_h[x]\n",
    "                else:\n",
    "                    general_h[x][x] = '?'\n",
    "    indices = [i for i,val in enumerate(general_h) if val==['?' for i in range(len(specific_h))]]\n",
    "    for i in indices:\n",
    "        general_h.remove(['?' for i in range(len(specific_h))])\n",
    "    return specific_h, general_h\n",
    "s_final, g_final = learn(concepts, target)\n",
    "print(\"Final S:\", s_final)\n",
    "print(\"Final G:\", g_final)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree based ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoGain(P, N):\n",
    "    import math\n",
    "    return -P / (P + N) * math.log2(P / ( P + N)) - N / (P + N) * math.log2(N / (P + N))\n",
    "def insertNode(tree, addTo, Node):\n",
    "    for k, v in tree.items():\n",
    "        if isinstance(v, dict):\n",
    "            tree[k] = insertNode(v, addTo, Node)\n",
    "    if addTo in tree:\n",
    "        if isinstance(tree[addTo], dict):\n",
    "            tree[addTo][Node] = 'None'\n",
    "        else:\n",
    "            tree[addTo] = {Node:'None'}\n",
    "    return tree\n",
    "def insertConcept(tree, addTo, Node):\n",
    "    for k, v in tree.items():\n",
    "        if isinstance(v, dict):\n",
    "            tree[k] = insertConcept(v, addTo, Node)\n",
    "    if addTo in tree:\n",
    "        tree[addTo] = Node\n",
    "    return tree\n",
    "def getNextNode(data, AttributeList, concept, conceptVals, tree, addTo):\n",
    "    Total = data.shape[0]\n",
    "    if Total == 0:\n",
    "        return tree\n",
    "\n",
    "    countC = {}\n",
    "    for cVal in conceptVals:\n",
    "        dataCC = data[data[concept] == cVal]\n",
    "        countC[cVal] = dataCC.shape[0]\n",
    "\n",
    "    if countC[conceptVals[0]] == 0:\n",
    "        tree = insertConcept(tree, addTo, conceptVals[1])\n",
    "        return tree\n",
    "\n",
    "    if countC[conceptVals[1]] == 0:\n",
    "        tree = insertConcept(tree, addTo, conceptVals[0])\n",
    "        return tree\n",
    "    ClassEntropy = infoGain(countC[conceptVals[1]],countC[conceptVals[0]])\n",
    "    Attr = {}\n",
    "    for a in AttributeList:\n",
    "        Attr[a] = list(set(data[a]))\n",
    "    AttrCount = {}\n",
    "    EntropyAttr = {}\n",
    "    for att in Attr:\n",
    "        for vals in Attr [att]:\n",
    "            for c in conceptVals:\n",
    "                iData = data[data[att] == vals]\n",
    "                dataAtt = iData[iData[concept] == c]\n",
    "                AttrCount[c] = dataAtt.shape[0]\n",
    "            TotalInfo = AttrCount[conceptVals[1]] + AttrCount[conceptVals[0]]\n",
    "            if AttrCount[conceptVals[1]] == 0 or AttrCount[conceptVals[0]] == 0:\n",
    "                InfoGain=0\n",
    "            else:\n",
    "                InfoGain = infoGain(AttrCount[conceptVals[1]], AttrCount[conceptVals[0]])\n",
    "\n",
    "            if att not in EntropyAttr:\n",
    "                EntropyAttr[att] = ( TotalInfo / Total ) * InfoGain\n",
    "            else:\n",
    "                EntropyAttr[att] = EntropyAttr[att] + ( TotalInfo / Total ) * InfoGain\n",
    "\n",
    "    Gain = {}\n",
    "    for g in EntropyAttr:\n",
    "        Gain[g] = ClassEntropy - EntropyAttr[g]\n",
    "    Node = max(Gain, key = Gain.get)\n",
    "    tree = insertNode(tree, addTo, Node)\n",
    "    for nD in Attr[Node]:\n",
    "        tree = insertNode(tree, Node, nD)\n",
    "        newData = data[data[Node] == nD].drop(Node, axis = 1)\n",
    "        AttributeList=list(newData)[:-1]\n",
    "        tree = getNextNode(newData, AttributeList, concept, conceptVals, tree, nD)\n",
    "    return tree\n",
    "def main():\n",
    "    from pandas import DataFrame\n",
    "    data = DataFrame.from_csv('PlayTennis.csv')\n",
    "    print(data)\n",
    "    AttributeList = list(data)[:-1]\n",
    "    concept = str(list(data)[-1])\n",
    "    conceptVals = list(set(data[concept]))\n",
    "    tree = getNextNode(data, AttributeList, concept, conceptVals, {'root':'None'}, 'root')\n",
    "    print(tree)\n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "X = X/np.amax(X,axis=0)\n",
    "y = y/100\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "epoch=7000\n",
    "learning_rate=0.1\n",
    "inputlayer_neurons = 2\n",
    "hiddenlayer_neurons = 3\n",
    "output_neurons = 1\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wo=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bo=np.random.uniform(size=(1,output_neurons))\n",
    "for i in range(epoch):\n",
    "    net_h=np.dot(X,wh) + bh\n",
    "    sigma_h= sigmoid(net_h)\n",
    "    net_o= np.dot(sigma_h,wo)+ bo\n",
    "    output = sigmoid(net_o)\n",
    "    deltaK = (y-output)* derivatives_sigmoid(output)\n",
    "    deltaH = deltaK.dot(wo.T) * derivatives_sigmoid(sigma_h)\n",
    "    wo = wo + sigma_h.T.dot(deltaK) *learning_rate\n",
    "    wh = wh + X.T.dot(deltaH) *learning_rate\n",
    "print(\"Input: \\n\" + str(X))\n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\" ,output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na√Øve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probAttr(data,attr,val):\n",
    "    Total=data.shape[0]\n",
    "    cnt = len(data[data[attr] == val])\n",
    "    return cnt,cnt/Total\n",
    "def train(data,Attr,conceptVals,concept):\n",
    "    conceptProbs = {}\n",
    "    countConcept={}\n",
    "    for cVal in conceptVals:\n",
    "        countConcept[cVal],conceptProbs[cVal] = probAttr(data,concept,cVal)\n",
    "    AttrConcept = {}\n",
    "    probability_list = {}\n",
    "    for att in Attr: #Create a tree for attribute\n",
    "        AttrConcept[att] = {}\n",
    "        probability_list[att] = {}\n",
    "        for val in Attr[att]:\n",
    "            AttrConcept[att][val] = {}\n",
    "            a,probability_list[att][val] = probAttr(data,att,val)\n",
    "            for cVal in conceptVals:\n",
    "                dataTemp = data[data[att]==val]\n",
    "                AttrConcept[att][val][cVal] = len(dataTemp[dataTemp[concept] == cVal])/countConcept[cVal]\n",
    "\n",
    "    print(\"P(A) : \",conceptProbs,\"\\n\")\n",
    "    print(\"P(X/A) : \",AttrConcept,\"\\n\")\n",
    "    print(\"P(X) : \",probability_list,\"\\n\")\n",
    "    return conceptProbs,AttrConcept,probability_list\n",
    "def test(examples,Attr,concept_list,conceptProbs,AttrConcept,probability_list):\n",
    "    misclassification_count=0\n",
    "    Total = len(examples)\n",
    "    for ex in examples:\n",
    "        px={}\n",
    "        for a in Attr:\n",
    "            for x in ex:\n",
    "                for c in concept_list:\n",
    "                    if x in AttrConcept[a]:\n",
    "                        if c not in px:\n",
    "                            px[c] = conceptProbs[c]*AttrConcept[a][x][c]/probability_list[a][x]\n",
    "                        else:\n",
    "                            px[c] = px[c]*AttrConcept[a][x][c]/probability_list[a][x]\n",
    "        print(px)\n",
    "        classification = max(px,key=px.get)\n",
    "        print(\"Classification :\",classification,\"Expected :\",ex[-1])\n",
    "        if(classification!=ex[-1]):\n",
    "            misclassification_count+=1\n",
    "    misclassification_rate=misclassification_count*100/Total\n",
    "    accuracy=100-misclassification_rate\n",
    "    print(\"Misclassification Count={}\".format(misclassification_count))\n",
    "    print(\"Misclassification Rate={}%\".format(misclassification_rate))\n",
    "    print(\"Accuracy={}%\".format(accuracy))\n",
    "def main():\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "    data = DataFrame.from_csv('PlayTennis_train1.csv')\n",
    "    concept=str(list(data)[-1])\n",
    "    concept_list = set(data[concept])\n",
    "    Attr={}\n",
    "    for a in list(data)[:-1]:\n",
    "        Attr[a] = set(data[a])\n",
    "    conceptProbs,AttrConcept,probability_list = train(data,Attr,concept_list,concept)\n",
    "    examples = DataFrame.from_csv('PlayTennis_test1.csv')\n",
    "    test(examples.values,Attr,concept_list,conceptProbs,AttrConcept,probability_list)\n",
    "main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na√Øve Bayes Classifier using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "msg = pd.read_csv('document.csv', names=['message', 'label'])\n",
    "print(\"Total Instances of Dataset: \", msg.shape[0])\n",
    "msg['labelnum'] = msg.label.map({'pos': 1, 'neg': 0})\n",
    "X = msg.message\n",
    "y = msg.labelnum\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)\n",
    "count_v = CountVectorizer()\n",
    "Xtrain_dm = count_v.fit_transform(Xtrain)\n",
    "Xtest_dm = count_v.transform(Xtest)\n",
    "df = pd.DataFrame(Xtrain_dm.toarray(),columns=count_v.get_feature_names())\n",
    "print(df[0:5])\n",
    "clf = MultinomialNB()\n",
    "clf.fit(Xtrain_dm, ytrain)\n",
    "pred = clf.predict(Xtest_dm)\n",
    "for doc, p in zip(Xtrain, pred):\n",
    "    p = 'pos' if p == 1 else 'neg'\n",
    "    print(\"%s -> %s\" % (doc, p))\n",
    "print('Accuracy Metrics: \\n')\n",
    "print('Accuracy: ', accuracy_score(ytest, pred))\n",
    "print('Recall: ', recall_score(ytest, pred))\n",
    "print('Precision: ', precision_score(ytest, pred))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(ytest, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "cancer_model = BayesianModel([('Pollution', 'Cancer'),('Smoker', 'Cancer'),('Cancer', 'Xray'),('Cancer',\n",
    "'Dyspnoea')])\n",
    "cancer_model.nodes()\n",
    "cancer_model.edges()\n",
    "cancer_model.get_cpds()\n",
    "cpd_poll = TabularCPD(variable='Pollution', variable_card=2, values=[[0.9], [0.1]])\n",
    "cpd_smoke = TabularCPD(variable='Smoker', variable_card=2, values=[[0.3], [0.7]])\n",
    "cpd_cancer = TabularCPD(variable='Cancer', variable_card=2, values=[[0.03, 0.05, 0.001, 0.02],\n",
    " [0.97, 0.95, 0.999, 0.98]],evidence=['Smoker', 'Pollution'], evidence_card=[2, 2])\n",
    "cpd_xray = TabularCPD(variable='Xray', variable_card=2, values=[[0.9, 0.2], [0.1, 0.8]],\n",
    " evidence=['Cancer'], evidence_card=[2])\n",
    "cpd_dysp = TabularCPD(variable='Dyspnoea', variable_card=2, values=[[0.65, 0.3], [0.35, 0.7]],\n",
    " evidence=['Cancer'], evidence_card=[2])\n",
    "cancer_model.add_cpds(cpd_poll, cpd_smoke, cpd_cancer, cpd_xray, cpd_dysp)\n",
    "cancer_model.check_model()\n",
    "cancer_model.get_cpds()\n",
    "print(cancer_model.get_cpds('Pollution'))\n",
    "print(cancer_model.get_cpds('Smoker'))\n",
    "print(cancer_model.get_cpds('Xray'))\n",
    "print(cancer_model.get_cpds('Dyspnoea'))\n",
    "print(cancer_model.get_cpds('Cancer'))\n",
    "cancer_model.local_independencies('Xray')\n",
    "cancer_model.local_independencies('Pollution')\n",
    "cancer_model.local_independencies('Smoker')\n",
    "cancer_model.local_independencies('Dyspnoea')\n",
    "cancer_model.local_independencies('Cancer')\n",
    "cancer_model.get_independencies()\n",
    "cancer_infer = VariableElimination(cancer_model)\n",
    "q = cancer_infer.query(variables=['Cancer'], evidence={'Smoker': 1})\n",
    "print(q['Cancer'])\n",
    "q = cancer_infer.query(variables=['Cancer'], evidence={'Smoker': 1,'Pollution': 1})\n",
    "print(q['Cancer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnosis of heart patients using standard Heart Disease Data Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt # Visuals\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "Cleveland_data_URL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/heartdisease/processed.hungarian.data'\n",
    "np.set_printoptions(threshold=np.nan) #see a whole array when we output it\n",
    "names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal','heartdisease']\n",
    "heartDisease = pd.read_csv(urlopen(Cleveland_data_URL), names = names) #gets Cleveland data\n",
    "del heartDisease['ca']\n",
    "del heartDisease['slope']\n",
    "del heartDisease['thal']\n",
    "del heartDisease['oldpeak']\n",
    "heartDisease = heartDisease.replace('?', np.nan)\n",
    "model = BayesianModel([('age', 'trestbps'), ('age', 'fbs'), ('sex', 'trestbps'), ('sex', 'trestbps'),('exang', 'trestbps'),('trestbps','heartdisease'),('fbs','heartdisease'),('heartdisease','restecg'),('heartdisease','thalach'),('heartdisease','chol')])\n",
    "model.fit(heartDisease, estimator=MaximumLikelihoodEstimator)\n",
    "print(model.get_cpds('age'))\n",
    "print(model.get_cpds('chol'))\n",
    "print(model.get_cpds('sex'))\n",
    "model.get_independencies()\n",
    "HeartDisease_infer = VariableElimination(model)\n",
    "q = HeartDisease_infer.query(variables=['heartdisease'], evidence={'age': 28})\n",
    "print(q['heartdisease'])\n",
    "q = HeartDisease_infer.query(variables=['heartdisease'], evidence={'chol': 100})\n",
    "print(q['heartdisease'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using EM Algorithm & k-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_iris\n",
    "import sklearn.metrics as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dataset=load_iris()\n",
    "X=pd.DataFrame(dataset.data)\n",
    "X.columns=['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
    "y=pd.DataFrame(dataset.target)\n",
    "y.columns=['Targets']\n",
    "plt.figure(figsize=(14,7))\n",
    "colormap=np.array(['red','lime','black'])\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y.Targets],s=40)\n",
    "plt.title('Real')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "model=KMeans(n_clusters=3)\n",
    "model.fit(X)\n",
    "predY=np.choose(model.labels_,[0,1,2]).astype(np.int64)\n",
    "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[predY],s=40)\n",
    "plt.title('KMeans')\n",
    "\n",
    "scaler=preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "xsa=scaler.transform(X)\n",
    "xs=pd.DataFrame(xsa,columns=X.columns)\n",
    "gmm=GaussianMixture(n_components=3)\n",
    "gmm.fit(xs)\n",
    "y_cluster_gmm=gmm.predict(xs)\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y_cluster_gmm],s=40)\n",
    "plt.title('GMM Classification')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbour Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "dataset=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(dataset[\"data\"],dataset[\"target\"],random_state=0)\n",
    "clf=KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X_train,y_train)\n",
    "for i in range(len(X_test)):\n",
    "    x=X_test[i]\n",
    "    x_new=np.array([x])\n",
    "    prediction=clf.predict(x_new)\n",
    "\n",
    "print(\"TARGET=\",y_test[i],dataset[\"target_names\"][y_test[i]],\"PREDICTED=\",prediction,dataset[\"target_names\"][prediction])\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Weighted Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "def lowess(x, y, f, iterations):\n",
    "    n = len(x)\n",
    "    r = int(ceil(f * n))\n",
    "    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]\n",
    "    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)\n",
    "    w = (1 - w ** 3) ** 3\n",
    "    yest = np.zeros(n)\n",
    "    delta = np.ones(n)\n",
    "    for iteration in range(iterations):\n",
    "        for i in range(n):\n",
    "            weights = delta * w[:, i]\n",
    "            b = np.array([np.sum(weights * y), np.sum(weights * y * x)])\n",
    "            A = np.array([[np.sum(weights), np.sum(weights * x)],[np.sum(weights * x), np.sum(weights * x * x)]])\n",
    "            beta = linalg.solve(A, b)\n",
    "            yest[i] = beta[0] + beta[1] * x[i]\n",
    "        residuals = y - yest\n",
    "        s = np.median(np.abs(residuals))\n",
    "        delta = np.clip(residuals / (6.0 * s), -1, 1)\n",
    "        delta = (1 - delta ** 2) ** 2\n",
    "    return yest\n",
    "def main():\n",
    " import math\n",
    " n = 100\n",
    " x = np.linspace(0, 2 * math.pi, n)\n",
    " y = np.sin(x) + 0.3 * np.random.randn(n)\n",
    " f =0.25\n",
    " iterations=3\n",
    " yest = lowess(x, y, f, iterations)\n",
    "\n",
    " import matplotlib.pyplot as plt\n",
    " plt.plot(x,y,\"r.\")\n",
    " plt.plot(x,yest,\"b-\")\n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8fd8fff81d0ee588fa822dd596bdc1ae902fd3ccef62f2b4a8ca9f32610e147"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
